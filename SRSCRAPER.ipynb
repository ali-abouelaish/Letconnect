{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMnJJRx4XtBq",
        "outputId": "68f584bf-41dc-4311-e224-d571885b68de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped 161 listings and saved to listings.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Input and output file paths\n",
        "input_csv = \"profiles.csv\"     # contains profile URLs in one column\n",
        "output_csv = \"listings.csv\"    # output file with scraped links\n",
        "\n",
        "# Read profile URLs from CSV (assuming first column has the URLs)\n",
        "profiles = pd.read_csv(input_csv, header=None)[0].tolist()\n",
        "\n",
        "base_url = \"https://www.spareroom.co.uk\"\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for profile_url in profiles:\n",
        "    try:\n",
        "        response = requests.get(profile_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Extract links from listing cards\n",
        "        for a in soup.find_all(\"a\", class_=\"listing-card__link\"):\n",
        "            href = a.get(\"href\")\n",
        "            if href:\n",
        "                full_url = href if href.startswith(\"http\") else base_url + href\n",
        "                all_results.append({\"profile\": profile_url, \"listing\": full_url})\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {profile_url}: {e}\")\n",
        "\n",
        "# Save results to CSV\n",
        "df = pd.DataFrame(all_results)\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"Scraped {len(all_results)} listings and saved to {output_csv}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "udtuxbbR3epB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and normalize text data (for titles, small fields)\"\"\"\n",
        "    if not text or text == \"N/A\":\n",
        "        return None\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())  # remove extra whitespace\n",
        "    text = re.sub(r'<[^>]+>', '', text)       # remove HTML tags\n",
        "    if len(text) > 500:\n",
        "        text = text[:500] + \"...\"\n",
        "    return text\n",
        "\n",
        "def extract_rich_text(elem):\n",
        "    \"\"\"Extract text while preserving emojis and newlines from <br> tags (for description only)\"\"\"\n",
        "    if not elem:\n",
        "        return None\n",
        "    for br in elem.find_all(\"br\"):\n",
        "        br.replace_with(\"\\n\")\n",
        "    text = elem.get_text()\n",
        "    text = text.strip()\n",
        "    return text if text else None\n",
        "\n",
        "def extract_price(price_text):\n",
        "    \"\"\"Extract and normalize price information (convert pw → monthly if needed)\"\"\"\n",
        "    if not price_text or price_text == \"N/A\":\n",
        "        return None\n",
        "\n",
        "    price_match = re.search(r'£?(\\d+(?:,\\d+)?(?:\\.\\d{2})?)', str(price_text))\n",
        "    if price_match:\n",
        "        price = price_match.group(1).replace(',', '')\n",
        "        try:\n",
        "            price = float(price)\n",
        "            text_lower = price_text.lower()\n",
        "\n",
        "            if \"pw\" in text_lower or \"per week\" in text_lower:\n",
        "                # Convert weekly → monthly (average 52 weeks / 12 months)\n",
        "                price = round(price * 52 / 12)\n",
        "\n",
        "            else:\n",
        "                # Assume price is already per month\n",
        "                price = round(price)\n",
        "\n",
        "            return int(price)\n",
        "        except:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def extract_coordinates(lat_text, lon_text):\n",
        "    \"\"\"Extract and validate coordinates\"\"\"\n",
        "    try:\n",
        "        lat = float(lat_text) if lat_text and lat_text != \"N/A\" else None\n",
        "        lon = float(lon_text) if lon_text and lon_text != \"N/A\" else None\n",
        "        if lat is not None and (-90 <= lat <= 90):\n",
        "            lat = round(lat, 8)\n",
        "        else:\n",
        "            lat = None\n",
        "        if lon is not None and (-180 <= lon <= 180):\n",
        "            lon = round(lon, 8)\n",
        "        else:\n",
        "            lon = None\n",
        "        return lat, lon\n",
        "    except:\n",
        "        return None, None\n",
        "\n",
        "def extract_feature_list(soup):\n",
        "    \"\"\"Extracts key-value pairs from feature-list <dl> blocks\"\"\"\n",
        "    features = {}\n",
        "    for dl in soup.find_all(\"dl\", class_=\"feature-list\"):\n",
        "        keys = dl.find_all(\"dt\", class_=\"feature-list__key\")\n",
        "        vals = dl.find_all(\"dd\", class_=\"feature-list__value\")\n",
        "\n",
        "        for k, v in zip(keys, vals):\n",
        "            key = clean_text(k.get_text()) if k else None\n",
        "            val = clean_text(v.get_text()) if v else None\n",
        "\n",
        "            # Handle tick/cross spans (\"Yes\"/\"No\")\n",
        "            if v and v.find(\"span\", class_=\"tick\"):\n",
        "                val = \"Yes\"\n",
        "            elif v and v.find(\"span\", class_=\"cross\"):\n",
        "                val = \"No\"\n",
        "\n",
        "            if key:\n",
        "                features[key] = val\n",
        "    return features\n",
        "\n",
        "def scrape_listing_advanced(url):\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',\n",
        "        }\n",
        "        resp = requests.get(url, headers=headers)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "        # Title\n",
        "        title = \"N/A\"\n",
        "        title_elem = soup.find(\"h1\") or soup.find(\"h2\") or soup.find(\"title\")\n",
        "        if title_elem:\n",
        "            title = clean_text(title_elem.get_text())\n",
        "\n",
        "        # Agent / Landlord name\n",
        "        agent_name = None\n",
        "        agent_elem = soup.find(\"strong\", class_=\"profile-photo__name\")\n",
        "        if agent_elem:\n",
        "            agent_name = clean_text(agent_elem.get_text())\n",
        "\n",
        "       # Extract location (always the 2nd <li> inside .key-features)\n",
        "        location = None\n",
        "        key_features = soup.find(\"ul\", class_=\"key-features\")\n",
        "        if key_features:\n",
        "         items = key_features.find_all(\"li\", class_=\"key-features__feature\")\n",
        "         if len(items) >= 2:\n",
        "          location = items[1].get_text(strip=True)  # ✅ \"Devons Road\"\n",
        "\n",
        "\n",
        "        # Latitude / Longitude\n",
        "        latitude, longitude = \"N/A\", \"N/A\"\n",
        "        script_tags = soup.find_all(\"script\")\n",
        "        for script in script_tags:\n",
        "            if script.string:\n",
        "                script_content = script.string\n",
        "                lat_match = re.search(r'latitude[\"\\s]*:[\"\\s]*\"?([0-9.-]+)\"?', script_content)\n",
        "                lon_match = re.search(r'longitude[\"\\s]*:[\"\\s]*\"?([0-9.-]+)\"?', script_content)\n",
        "                if lat_match:\n",
        "                    latitude = lat_match.group(1)\n",
        "                if lon_match:\n",
        "                    longitude = lon_match.group(1)\n",
        "                location_match = re.search(\n",
        "                    r'location[\"\\s]*:[\"\\s]*{[^}]*latitude[\"\\s]*:[\"\\s]*\"?([0-9.-]+)\"?[^}]*longitude[\"\\s]*:[\"\\s]*\"?([0-9.-]+)\"?',\n",
        "                    script_content\n",
        "                )\n",
        "                if location_match:\n",
        "                    latitude, longitude = location_match.group(1), location_match.group(2)\n",
        "\n",
        "        # Photos (only inside photo-gallery containers)\n",
        "        all_photo_urls = []\n",
        "\n",
        "        # ✅ Main image container\n",
        "        main_gallery = soup.select_one(\"dl.photo-gallery__main-image-wrapper\")\n",
        "        if main_gallery:\n",
        "            main_links = main_gallery.find_all(\"a\", href=re.compile(\n",
        "                r\"^https://photos2\\.spareroom\\.co\\.uk/images/flatshare/listings/large/[0-9]+/[0-9]+/[0-9]+\\.jpg$\"\n",
        "            ))\n",
        "            for link in main_links:\n",
        "                photo_url = link.get(\"href\")\n",
        "                if photo_url and photo_url not in all_photo_urls:\n",
        "                    all_photo_urls.append(photo_url)\n",
        "\n",
        "        # ✅ Thumbnail gallery container\n",
        "        thumb_gallery = soup.select_one(\"div.photo-gallery__thumbnails\")\n",
        "        if thumb_gallery:\n",
        "            thumb_links = thumb_gallery.find_all(\"a\", href=re.compile(\n",
        "                r\"^https://photos2\\.spareroom\\.co\\.uk/images/flatshare/listings/large/[0-9]+/[0-9]+/[0-9]+\\.jpg$\"\n",
        "            ))\n",
        "            for link in thumb_links:\n",
        "                photo_url = link.get(\"href\")\n",
        "                if photo_url and photo_url not in all_photo_urls:\n",
        "                    all_photo_urls.append(photo_url)\n",
        "\n",
        "        first_photo_url = all_photo_urls[0] if all_photo_urls else None\n",
        "        photo_count = len(all_photo_urls)\n",
        "        all_photos = \", \".join(all_photo_urls) if all_photo_urls else None\n",
        "\n",
        "\n",
        "\n",
        "        # Price\n",
        "        price = None\n",
        "        price_selectors = [\".price\", \".rent\", \".amount\", \"[class*='price']\", \"[class*='rent']\", \"[class*='amount']\"]\n",
        "        for selector in price_selectors:\n",
        "            try:\n",
        "                elements = soup.select(selector)\n",
        "                for elem in elements:\n",
        "                    text = elem.get_text(strip=True)\n",
        "                    if '£' in text:\n",
        "                        price = extract_price(text)\n",
        "                        if price:\n",
        "                            break\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # ✅ Description (preserve emojis + newlines)\n",
        "        description = None\n",
        "        detaildesc_elem = soup.find(\"p\", class_=\"detaildesc\")\n",
        "        if detaildesc_elem:\n",
        "            description = extract_rich_text(detaildesc_elem)\n",
        "        else:\n",
        "            feature_desc_body = soup.find(\"div\", class_=\"feature__description-body\")\n",
        "            if feature_desc_body:\n",
        "                description = extract_rich_text(feature_desc_body)\n",
        "            else:\n",
        "                desc_selectors = [\n",
        "                    \".description\", \".details\", \".content\",\n",
        "                    \"[class*='description']\", \"[class*='details']\",\n",
        "                    \"[class*='content']\", \"p\", \".listing-details\"\n",
        "                ]\n",
        "                for selector in desc_selectors:\n",
        "                    try:\n",
        "                        elements = soup.select(selector)\n",
        "                        for elem in elements:\n",
        "                            text = extract_rich_text(elem)\n",
        "                            if text and len(text) > 50:\n",
        "                                description = text\n",
        "                                break\n",
        "                        if description:\n",
        "                            break\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "        # Property type\n",
        "        property_type = None\n",
        "        type_keywords = [\"room\", \"bedroom\", \"studio\", \"flat\", \"apartment\", \"house\", \"en-suite\", \"ensuite\"]\n",
        "        search_text = (str(title) + \" \" + str(description)).lower()\n",
        "        for keyword in type_keywords:\n",
        "            if keyword in search_text:\n",
        "                property_type = keyword.title()\n",
        "                break\n",
        "\n",
        "        # ✅ Extract structured features\n",
        "        features = extract_feature_list(soup)\n",
        "        available_date = features.get(\"Available\")\n",
        "        min_term = features.get(\"Minimum term\")\n",
        "        max_term = features.get(\"Maximum term\")\n",
        "        deposit = features.get(\"Deposit\")\n",
        "        bills_included = features.get(\"Bills included?\")\n",
        "        furnishings = features.get(\"Furnishings\")\n",
        "        parking = features.get(\"Parking\")\n",
        "        garden = features.get(\"Garden/patio\")\n",
        "        broadband = features.get(\"Broadband included\")\n",
        "        housemates = features.get(\"# housemates\")\n",
        "        total_rooms = features.get(\"Total # rooms\")\n",
        "        smoker = features.get(\"Smoker?\")\n",
        "        pets = features.get(\"Any pets?\")\n",
        "        occupation = features.get(\"Occupation\")\n",
        "        gender = features.get(\"Gender\")\n",
        "        couples_ok = features.get(\"Couples OK?\")\n",
        "        smoking_ok = features.get(\"Smoking OK?\")\n",
        "        pets_ok = features.get(\"Pets OK?\")\n",
        "        pref_occupation = features.get(\"Occupation\")  # new housemate occupation\n",
        "        references = features.get(\"References?\")\n",
        "        min_age = features.get(\"Min age\")\n",
        "        max_age = features.get(\"Max age\")\n",
        "\n",
        "        # Final result dictionary\n",
        "        lat, lon = extract_coordinates(latitude, longitude)\n",
        "        result = {\n",
        "            \"url\": url,\n",
        "            \"title\": title,\n",
        "            \"agent_name\": agent_name,\n",
        "            \"location\": location,\n",
        "            \"latitude\": lat,\n",
        "            \"longitude\": lon,\n",
        "            \"status\": \"available\",\n",
        "            \"price\": price,\n",
        "            \"description\": description,  # ✅ emojis + newlines preserved\n",
        "            \"property_type\": property_type,\n",
        "            \"available_date\": available_date,\n",
        "            \"min_term\": min_term,\n",
        "            \"max_term\": max_term,\n",
        "            \"deposit\": deposit,\n",
        "            \"bills_included\": bills_included,\n",
        "            \"furnishings\": furnishings,\n",
        "            \"parking\": parking,\n",
        "            \"garden\": garden,\n",
        "            \"broadband\": broadband,\n",
        "            \"housemates\": housemates,\n",
        "            \"total_rooms\": total_rooms,\n",
        "            \"smoker\": smoker,\n",
        "            \"pets\": pets,\n",
        "            \"occupation\": occupation,\n",
        "            \"gender\": gender,\n",
        "            \"couples_ok\": couples_ok,\n",
        "            \"smoking_ok\": smoking_ok,\n",
        "            \"pets_ok\": pets_ok,\n",
        "            \"pref_occupation\": pref_occupation,\n",
        "            \"references\": references,\n",
        "            \"min_age\": min_age,\n",
        "            \"max_age\": max_age,\n",
        "            \"photo_count\": photo_count,\n",
        "            \"first_photo_url\": first_photo_url,\n",
        "            \"all_photos\": all_photos,\n",
        "            \"photos\": json.dumps(all_photo_urls) if all_photo_urls else None,\n",
        "        }\n",
        "\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to scrape {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def open_csv(filename):\n",
        "    \"\"\"Try UTF-8 first, fallback to latin-1\"\"\"\n",
        "    try:\n",
        "        return open(filename, newline=\"\", encoding=\"utf-8\")\n",
        "    except UnicodeDecodeError:\n",
        "        return open(filename, newline=\"\", encoding=\"latin-1\")\n",
        "\n",
        "def main():\n",
        "    results = []\n",
        "    with open(\"listings.csv\", newline=\"\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        for row in reader:\n",
        "            if len(row) < 2:  # skip rows without a second column\n",
        "                continue\n",
        "            url = row[1].strip()   # ✅ use second column instead of first\n",
        "            if not url:\n",
        "                continue\n",
        "            print(f\"Scraping {url}\")\n",
        "            data = scrape_listing_advanced(url)\n",
        "            if data:\n",
        "                results.append(data)\n",
        "            time.sleep(1)  # be polite\n",
        "\n",
        "    # Columns to save\n",
        "    database_columns = [\n",
        "        \"url\", \"title\", \"agent_name\", \"location\", \"latitude\", \"longitude\", \"status\", \"price\",\n",
        "        \"description\", \"property_type\", \"available_date\", \"min_term\", \"max_term\",\n",
        "        \"deposit\", \"bills_included\", \"furnishings\", \"parking\", \"garden\",\n",
        "        \"broadband\", \"housemates\", \"total_rooms\", \"smoker\", \"pets\", \"occupation\",\n",
        "        \"gender\", \"couples_ok\", \"smoking_ok\", \"pets_ok\", \"pref_occupation\",\n",
        "        \"references\", \"min_age\", \"max_age\", \"photo_count\", \"first_photo_url\",\n",
        "        \"all_photos\", \"photos\"\n",
        "    ]\n",
        "\n",
        "    output_filename = \"newscrape.csv\"\n",
        "    with open(output_filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=database_columns)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(results)\n",
        "\n",
        "    print(f\"Scraping complete. Results saved to {output_filename}\")\n",
        "    print(f\"Successfully scraped {len(results)} properties from all_vac.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuAxqAzfW0EA",
        "outputId": "408c2404-4b4a-4e0b-baba-7fb98e6436e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping listing\n",
            "Failed to scrape listing: Invalid URL 'listing': No scheme supplied. Perhaps you meant https://listing?\n",
            "Scraping https://www.spareroom.co.uk/flatshare/flatshare_detail.pl?flatshare_id=16801986&search_id=&city_id=&flatshare_type=offered&search_results=%2Fu2901752&\n",
            "Scraping https://www.spareroom.co.uk/flatshare/flatshare_detail.pl?flatshare_id=17984292&search_id=&city_id=&flatshare_type=offered&search_results=%2Fu2901752&\n",
            "Scraping https://www.spareroom.co.uk/flatshare/flatshare_detail.pl?flatshare_id=17999696&search_id=&city_id=&flatshare_type=offered&search_results=%2Fu2901752&\n",
            "Scraping https://www.spareroom.co.uk/flatshare/flatshare_detail.pl?flatshare_id=15708796&search_id=&city_id=&flatshare_type=offered&search_results=%2Fu2901752&\n",
            "Scraping https://www.spareroom.co.uk/flatshare/flatshare_detail.pl?flatshare_id=16767096&search_id=&city_id=&flatshare_type=offered&search_results=%2Fu2901752&\n",
            "Scraping https://www.spareroom.co.uk/flatshare/flatshare_detail.pl?flatshare_id=16983333&search_id=&city_id=&flatshare_type=offered&search_results=%2Fu2901752&\n",
            "Scraping https://www.spareroom.co.uk/flatshare/flatshare_detail.pl?flatshare_id=16958779&search_id=&city_id=&flatshare_type=offered&search_results=%2Fu2901752&\n",
            "Scraping https://www.spareroom.co.uk/flatshare/flatshare_detail.pl?flatshare_id=17061746&search_id=&city_id=&flatshare_type=offered&search_results=%2Fu2901752&\n",
            "Scraping https://www.spareroom.co.uk/flatshare/flatshare_detail.pl?flatshare_id=16520146&search_id=&city_id=&flatshare_type=offered&search_results=%2Fu2901752&\n",
            "Scraping https://www.spareroom.co.uk/flatshare/flatshare_detail.pl?flatshare_id=17089973&search_id=&city_id=&flatshare_type=offered&search_results=%2Fu2901752&\n",
            "Scraping https://www.spareroom.co.uk/flatshare/flatshare_detail.pl?flatshare_id=17977158&search_id=&city_id=&flatshare_type=offered&search_results=%2Fu15868888&\n",
            "Scraping https://www.spareroom.co.uk/flatshare/flatshare_detail.pl?flatshare_id=17968570&search_id=&city_id=&flatshare_type=offered&search_results=%2Fu15868888&\n",
            "Scraping https://www.spareroom.co.uk/flatshare/flatshare_detail.pl?flatshare_id=17991198&search_id=&city_id=&flatshare_type=offered&search_results=%2Fu15868888&\n",
            "Scraping https://www.spareroom.co.uk/flatshare/flatshare_detail.pl?flatshare_id=17982695&search_id=&city_id=&flatshare_type=offered&search_results=%2Fu15868888&\n",
            "Scraping https://www.spareroom.co.uk/flatshare/flatshare_detail.pl?flatshare_id=17995087&search_id=&city_id=&flatshare_type=offered&search_results=%2Fu15868888&\n",
            "Scraping https://www.spareroom.co.uk/flatshare/flatshare_detail.pl?flatshare_id=17944829&search_id=&city_id=&flatshare_type=offered&search_results=%2Fu15868888&\n",
            "Scraping https://www.spareroom.co.uk/flatshare/flatshare_detail.pl?flatshare_id=17968586&search_id=&city_id=&flatshare_type=offered&search_results=%2Fu15868888&\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d4jktbxiXvWQ"
      }
    }
  ]
}